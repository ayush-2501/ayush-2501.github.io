<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ayush Pradhan</title>
    <description>Ayush Pradhan&apos;s Online Portfolio.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 07 May 2023 16:44:54 +0530</pubDate>
    <lastBuildDate>Sun, 07 May 2023 16:44:54 +0530</lastBuildDate>
    <generator>Jekyll v3.9.3</generator>
    
      <item>
        <title>Google File System (GFS) / Colossus</title>
        <description>&lt;p&gt;Reference papers:&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf&quot;&gt;The Google File System&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://cacm.acm.org/magazines/2010/3/76283-gfs-evolution-on-fast-forward/fulltext&quot;&gt;GFS: Evolution on Fast-Forward&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;One of the main challenges in the domain of distributed systems is building scalable storage systems that can handle large amounts of data in an efficient way. The Google File System (GFS) proposed in [1] is a scalable distributed file system for large scale data-intensive applications. Ghemawat et al. highlight some of the key design and performance considerations that went into developing GFS. Moreover, they emphasize on the fact that although the system is built keeping in mind Google’s unique setting, these ideas are also applicable to similar systems. The authors of [1] start by stating a few assumptions that influence their design decisions – system failures are not an exception but a norm, most of the writes are append operations, larger block sizes are more efficient and throughput is more important than low-latency. The authors then describe the overall architecture of GFS that includes a single master and multiple distributed chunk servers. The master is a central repository that stores the metadata about each file and the chunk servers responsible for those files. The actual data is stored in chunks of 64MB which is distributed amongst the chunk servers. In order to prevent the master from being a bottleneck, the system is designed such that no data transfer involves the master, the clients directly fetch data from the chunk servers. Keeping fault-tolerance and availability in mind, the system also employs a heartbeat mechanism for health checks, and each chunk server is replicated 3 times (configurable). The authors of [1] also discuss design aspects such as replication strategies, garbage collection, failure recovery through snapshots and checkpoints and handling concurrent writes. Finally, the authors provide experimental results of GFS performance in both, a simulated environment and real Google clusters running GFS in production. The results provide conclusive evidence that the system is able to handle reads and writes with a reasonably good performance at a large scale.
In [2] Quinlan provides deeper insights into how GFS has evolved and adapted to the changing demands of large- scale systems. The authors in [2] specifically discuss issues faced by GFS like the master being a single point of failure, number of files being a bottleneck and the trend of applications shifting towards low-latency requirements. They also discuss the advantages of another system –BigTable and how it is able to utilize GFS to provide efficient access to files of smaller sizes. Although, there were some hiccups in adopting GFS to the changing needs of servicing latency critical client facing applications, the overall impact of GFS was positive.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors of [1] made the paper very accessible by clearly stating their assumptions, using easy to follow illustrations of their design and providing metrics from real-world services provided by google. For example, the tables analyzing the operations and bytes transfer breakdown on Google clusters were very insightful.&lt;/li&gt;
  &lt;li&gt;In [1], the authors also discussed potential pitfalls of some of their design choices and provided useful alterna- tives. For instance, in section 2.5 they highlighted the issue of hotspots and provided an alternative solution of allowing clients to communicate with eachother.&lt;/li&gt;
  &lt;li&gt;In [1] the authors proposed a novel solution to the split-brain problem – the usage of lease given to a primary chunk server, and the expiration timeout that prevents assigning multiple primaries in case of a failure. This is an elegant solution to a difficult problem.&lt;/li&gt;
  &lt;li&gt;In [2], the authors discuss some of the limitations of GFS in retrospective that were not evident from the initial design proposed in [1]. For example, in [1] the authors ascertain that the cost of adding extra memory to the master is a small price to pay for the potential advantages of storing the metadata in memory. However, the discussion in [2] provides a different perspective and helps in better understanding of the drawbacks of some design choices introduced in [1].&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors in [1] elaborate on several concepts involving fault-tolerance and availability, but do not provide sufficient illustrations or pseudo-code samples for some complicated mechanisms like recovery using snapshots and checkpoints. The garbage collection process could have been explained in more detailed as it is an important aspect of a storage system.&lt;/li&gt;
  &lt;li&gt;In the experiments pertinent to fault-tolerance in section 6.2.5 of [1], the authors discuss recovery times for chunk server failures, but did not consider the scenario when a master server fails. This scenario is of particular importance in a single-master system. Provided some analysis of the effects of master failure would have been helpful.&lt;/li&gt;
  &lt;li&gt;In the related work section of [1], it would have been helpful to include a table or a concise summary of the different kinds of file system architectures, different kinds of workloads, and which architectures are and are not effective for each kind of workload.&lt;/li&gt;
  &lt;li&gt;In [2], the authors discuss the consistency issues that occur in GFS. With regards to the design proposed in [1], the consistency model was relaxed to simplify the design however, since consistency is generally an important requirement of storage systems, the authors should have made a more convincing case for their design choice.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;How can the GFS design be modified to support strong consistency? – Some things that can be introduced to the system are 2 Phase Commits (2PC) and all writes being done through the primary chunk server.&lt;/li&gt;
  &lt;li&gt;Active-active Masters: Can we use an active-active mechanism by having multiple Masters that balance the load amongst each other? What could be some potential drawbacks of this?&lt;/li&gt;
  &lt;li&gt;GFS is not really suitable for writes at random locations and focuses entirely on appends. Would it be possible to extend the architecture to support random writes to make it more general? Given that SSDs are readily available and allow fast random writes.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 10 Feb 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/file%20systems/software%20engineering/storage/2022/02/10/gfs.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/file%20systems/software%20engineering/storage/2022/02/10/gfs.html</guid>
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>File systems</category>
        
        <category>Software Engineering</category>
        
        <category>Storage</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>File systems</category>
        
        <category>Software Engineering</category>
        
        <category>Storage</category>
        
      </item>
    
      <item>
        <title>Consistent Hashing: Chord</title>
        <description>&lt;p&gt;Reference paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf&quot;&gt;Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;One of the fundamental problems in peer-to-peer (P2P) applications is to efficiently locate a particular node in the network. Stoica et al. present the design for a scalable and efficient P2P distributed lookup protocol named Chord. The authors first give an overview of consistent hashing which is widely used in such scenarios. They highlight the scalability issues of consistent hashing that are caused by each node having to maintain the state of all of the other nodes in the network. In order to overcome this limitation, they propose the use of distributed hash-based indexing and routing algorithms that allow Chord to handle a large number of nodes and data lookups efficiently. In the proposed protocol, each node only needs to maintain a reference to its immediate successor and only a subset of all other nodes in the network, leading to better scalability. Chord is also able to handle node join and leave operations without requiring significant changes to the network’s structure, which contributes to its robustness in dynamic networks. The authors provide a thorough analysis of the performance of Chord and show that it performs well in terms of time complexity that scales logarithmically with the number of Chord nodes. Lastly, the authors also prove the fault-tolerance capabilities of Chord through experiments.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors prefaced each experiment in section 6 with a motivation followed by experiment details and the results. This provided a deeper understanding of what questions the experiments were meant to answer, for instance, the experiments showing the distribution of keys per node in the server were prefaced by the motivation and hence the results lead to a deeper understanding of the system.&lt;/li&gt;
  &lt;li&gt;The authors explained the protocol design and implementation very clearly by providing sufficient illustrations and pseudo-code. Specifically, the illustration in figure 5 showing how the protocol handles node joins followed by the pseudo-code in figure 6 made the author’s ideas easy to follow.&lt;/li&gt;
  &lt;li&gt;Apart from introducing a novel protocol, the authors are spent effort in discussing/demonstrating the fault-tolerance aspects of their design which made it feasible for their protocol to be adapted into practical applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The algorithm used in the paper for generating hashes (SHA-1) has been proven to be insecure, thereby making this version of Chord more vulnerable to security risks. Although, at the time the paper was published SHA-1 was a commonly used hashing algorithm.&lt;/li&gt;
  &lt;li&gt;The paper fails to mention how many messages are required to properly initialize the state of the “ring” of m nodes when it first starts up. Further information about how to properly set-up the protocol would have made it easier to adopt Chord into practical applications.&lt;/li&gt;
  &lt;li&gt;Security, despite being a major consideration in P2P networks where virtually any node can connect and start transmitting messages, was only discussed briefly in the paper. The authors could have conducted experiments to test the solutions to security issues proposed in the last section.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Chord does not use network topology information to setup the connections between peers. Can we incorporate location information for each node to provide better latency? – eg. Tapestry protocol, Pastry protocol etc.&lt;/li&gt;
  &lt;li&gt;Lookups in chord are asymmetric because of the way connections between nodes is set up. For example, lookup from node n to node p could take a different number of hops than from node p to n. Could making symmetric connections (nodes p and n both point to each other) improve predictability?&lt;/li&gt;
  &lt;li&gt;The concept of unidirectional routing in Chord can be misused during a routing attack (where a malicious node purposely detours messages to increase path length). Can we modify the algorithm to include bi-directional edges and bi-directional finger table entries to prevent this?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 05 Feb 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/consistent%20hashing/software%20engineering/2022/02/05/chord.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/consistent%20hashing/software%20engineering/2022/02/05/chord.html</guid>
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>Consistent Hashing</category>
        
        <category>Software Engineering</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>Consistent Hashing</category>
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>Distributed systems: Stragglers</title>
        <description>&lt;p&gt;Reference paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2408776.2408794?download=true&quot;&gt;The Tail at Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;One of the challenges in building large-scale systems is to consistently maintain low-latency in responses. Dean and Barroso, in their paper highlight techniques that can be used to build distributed systems that have an overall low-latency despite its constituent components having occasional high-latency.
The authors first provide several reasons that contribute to high tail latency such as shared resources, queuing, daemons, garbage collection etc. and then they show that variability in latency of individual components lead to high overall service latency due to the “fan-out” architecture generally adopted in large-scale systems. The authors take inspiration from fault-tolerant systems and focus on techniques that reduce latency hiccups regardless of root cause. Finally, the authors introduce techniques such as sending the same request to multiple replicas and only considering the first response (hedged requests) and allowing multiple servers to communicate updates regarding requests (tied requests) to reduce tail latency. Other techniques proposed by the authors such as micro-partitioning, selective replication and latency-induced probation also lead to significant reduction in latency at larger scales.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors made the paper accessible to a wider audience by using sufficient examples and metrics from real-world services provided by google. For example, the graph showing how the probability of high-service latency scales with number of servers was very insightful.&lt;/li&gt;
  &lt;li&gt;In the last few sections of the paper, the authors provide important considerations to keep in mind while building production quality robust distributed systems such as canary requests. These techniques are also applicable to a broader range of services apart from information retrieval.&lt;/li&gt;
  &lt;li&gt;For several design techniques that the authors proposed, they highlighted various trade-offs that were made in adopting those techniques. For example, the good-enough results provided by google search and skipping non-essential subsystems. Such accounts provide a deeper understanding of the issues faced in large-scale systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;For some proposed techniques such as latency-induced probation and selective replication the authors do not carry out evaluations. It would have been helpful if they included some statistics about these techniques.&lt;/li&gt;
  &lt;li&gt;The authors do not discuss the implications that write operations would have on their proposed techniques. For example, if we issue two write operations in hedged requests, a change may be made twice.&lt;/li&gt;
  &lt;li&gt;The authors could have provided experiments that compare directly reducing component-level variability vs tail-tolerant techniques that mask or work around temporary latency pathologies to further enforce their points.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;How would latency-induced probation work in case of writing to a server on probation?&lt;/li&gt;
  &lt;li&gt;In a complex large-scale distributed system, there are a lot of moving parts. How can one identify issues that
are specially caused by latency variability? For example, some latency issues could be caused by faults/retries.&lt;/li&gt;
  &lt;li&gt;One of the modern commercial databases that use hedged requests is Cassandra. It uses the percentile heuristic to send 3 requests for each request. This increases the load on the server but provides better latency.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 24 Jan 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/networks/software%20engineering/2022/01/24/tail-at-scale.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/distributed%20systems/backend/networks/software%20engineering/2022/01/24/tail-at-scale.html</guid>
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>Networks</category>
        
        <category>Software Engineering</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Distributed Systems</category>
        
        <category>Backend</category>
        
        <category>Networks</category>
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>Advanced topics in server design</title>
        <description>&lt;p&gt;Reference papers:&lt;/p&gt;

&lt;p&gt;[1]  &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usenix99/full_papers/banga/banga.pdf&quot;&gt;A Scalable and Explicit Event Delivery Mechanism for UNIX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]  &lt;a href=&quot;https://www.usenix.org/legacy/event/usenix04/tech/general/full_papers/brecht/brecht.pdf&quot;&gt;accept()able Strategies for Improving Web Server Performance&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Server performance depends on a number of factors and in order to design high performance and scalable servers, it is important to carefully reason about these factors. Authors of [1] and [2] argue that a server’s policies for accepting new client connections [2] and the OS event-notification mechanism [1], have a signif- icant effect on a server’s performance. The authors in [1] highlight that web servers using the OS system call select (or poll) to check for events, scale poorly with the event rate. To mitigate the scalability issue they propose an event-based notification mechanism wherein the kernel maintains a queue of events and notifies the application once an event is ready. This leads to better scalability as the kernel performs work proportional to the number of events. The authors in [2] study another important aspect of server design – the policy for accepting new connections. Through their experiments, they argue that finding the correct balance between accepting new connections and processing existing connections can improve the performance of web servers. They conduct experiments on three different servers and vary the number of consecutively accepted connections using an accept-limit parameter – the results show that well-tuned accept policies yield reasonable improvements as compared to the baseline policy.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors in [1] explained the API design and implementation very clearly by providing sufficient illustrations and code-examples. The effort they put into profiling the kernel in order to compare the CPU utilization of their API and select provided great insights into the inefficiency of select.&lt;/li&gt;
  &lt;li&gt;The authors in [1] provided a survey of event-management APIs in Operating Systems other than UNIX and highlighted how the same ideas were already being successfully used in other Operating Systems like Windows NT.&lt;/li&gt;
  &lt;li&gt;The authors in [2] spent great effort in fairly comparing the TUX and μserver by configuring them to function under similar resource limits. This is an important point to consider while comparing different servers.&lt;/li&gt;
  &lt;li&gt;The authors in [2] devised an elaborate testing strategy and spent effort in highlighting the drawbacks of a commonly used testing workload. Moreover, they studied the server’s performances under a unique one-packet workload that provided deeper insights.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The authors in [1] claim that the poll system call shares the same scaling problem as select, however they only provide experimental results by comparing their API with select. An experiment showcasing poll performance as well, could back their claim.&lt;/li&gt;
  &lt;li&gt;In [1], the authors could have also studied the relationship between the array max parameter and response time to determine whether the increased costs due to increase in array max has any effect on the response time.&lt;/li&gt;
  &lt;li&gt;The authors in [2] reported the most interesting accept strategies in the experiments, however they did not elaborate on what alternative strategies they tried and why they chose a particular strategy among others.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Fine-tuning the optimal accept-rate for each server is a time consuming task. As showed in [2], better performance can be observed by dynamically adjusting the accept strategy – Can a “calibration” phase at the starting of a server be used to set an initial accept-limit, and then the load can be monitored in real-time to vary the accept-limit dynamically?&lt;/li&gt;
  &lt;li&gt;In [1], a fixed length queue is maintained per-thread for events pertaining to each file-descriptor. This queue can be replaced with a circular queue which could improve space complexity by making efficient use of free memory.&lt;/li&gt;
  &lt;li&gt;Will the event-notification API proposed in [1] provide similar improvements in a multi-processor environment as it does in a single processor setting? Would there be significant changes in the API implementation to support multi-processor events?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 20 Jan 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/servers/backend/networks/software%20engineering/2022/01/20/server-design-advanced.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/servers/backend/networks/software%20engineering/2022/01/20/server-design-advanced.html</guid>
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Servers</category>
        
        <category>Backend</category>
        
        <category>Networks</category>
        
        <category>Software Engineering</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Servers</category>
        
        <category>Backend</category>
        
        <category>Networks</category>
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>Flash: An efficient and portable Web Server</title>
        <description>&lt;p&gt;The reference paper can be found &lt;a href=&quot;https://www.usenix.org/legacy/events/usenix99/full_papers/pai/pai.pdf&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this paper, the authors propose Flash – an efficient and portable web server that utilizes an asymmetric multi-process even-driven (AMPED) architecture. This architecture is a combination of the single-process event-driven (SPED) and multi-process/multi-thread (MP/MT) architectures. Flash consists of a main process that handles HTTP requests in an event-driven fashion, and it uses several asynchronous helper processes to perform blocking operations on the disk. The authors provide a brief overview of the different types of server architectures and analyze their server’s performance compared to Zeus (SPED) and Apache (MP/MT) servers as benchmarks. The authors also introduce several important optimizations in caching, byte alignment, file access via mmap and checking for memory residency before calling helper processes which result in large performance gains. Most of these optimizations are fairly standard in present-day web servers but at the time the paper was published, these ideas were quite novel.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The paper provides great insights about key points to consider while designing web servers. It also provides a detailed comparison based on performance characteristics of each design choice.&lt;/li&gt;
  &lt;li&gt;The evaluation experiments are conducted in an exhaustive manner with carefully drawn out workloads that tested the server’s performance at its maximum capacity. The evaluation workflow is exemplary.&lt;/li&gt;
  &lt;li&gt;The paper introduces many optimization ideas that are widely used and applicable in present-day web servers. The ideas of application-level caching with both data and computation caches, DMA for scatter-gather support etc. are ideas that were quite novel at the time the paper was published.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;At the time this paper was published, most web servers mainly handled static data. However, in present day servers the content is increasingly dynamic. Therefore the claim that most non-blocking IO operations are just disk reads is not true anymore.&lt;/li&gt;
  &lt;li&gt;The paper does not talk about sendfile() which is a non-blocking system call that uses Direct Memory Access (DMA) to copy a file’s contents into OS memory directly.&lt;/li&gt;
  &lt;li&gt;The authors ran most of the evaluation experiments on in a LAN setting and their WAN evaluations were also simulated on LAN. Nowadays, most of the clients interact with servers through WAN.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Present day web-servers serve increasingly dynamic content – how would a server like Flash handle such work loads? Creating a new process for each dynamic request may not be feasible.&lt;/li&gt;
  &lt;li&gt;Why did Flash lose against competitors like Apache, which also have adopted many ideas from Flash?&lt;/li&gt;
  &lt;li&gt;Can sendfile() (and related syscalls) be used to handle blocking disk operations instead of multiple threads/processes? Are there any caveats?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/servers/backend/software%20engineering/2022/01/16/server-design-basic.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/servers/backend/software%20engineering/2022/01/16/server-design-basic.html</guid>
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Servers</category>
        
        <category>Backend</category>
        
        <category>Software Engineering</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>Servers</category>
        
        <category>Backend</category>
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>The End-to-End Principle in system design</title>
        <description>&lt;p&gt;The reference paper can be found &lt;a href=&quot;https://pages.cs.wisc.edu/~bart/739/papers/end-to-end.pdf&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this paper, the authors provide an argument that follows the “Occam’s razor” principle in the context of designing systems. The central problem that the authors address is deciding the appropriate boundaries between the functions of a distributed system, and where to place these functions in the system. The end- to-end argument aims to make the lower levels of a system as simple as possible by implementing functions closer to the endpoints. The authors do not propose the end-to-end argument as a strict rule, but as a set of rational principles to keep in mind while designing layered systems. This argument has contributed to the rapid growth of the Internet as we know it today (TCP/IP) and it is also found in several other applications such as end-to-end encryption, reliable file transmission etc.&lt;/p&gt;

&lt;h2 id=&quot;positive-points&quot;&gt;Positive Points&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The end-to-end argument keeps the core of a system relatively simpler as the majority of the complex logic is implemented by the application. This may sometimes also result in performance enhancements if the function implementation on the lower level is expensive.&lt;/li&gt;
  &lt;li&gt;The end-to-end argument makes the system modular and flexible as it allows highly specialized appli- cations to be built without changing the lower level systems. For example, TCP running on top of IP (layered systems), exo-kernels, RISC etc.&lt;/li&gt;
  &lt;li&gt;By definition, the lower layers in an end-to-end network contain coarse-grained states whereas the endpoints maintain the main states. Therefore, these systems can survive partial network failures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;drawbacks&quot;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Implementing the function at the “ends” of a system is not always feasible/possible. For example, issues in mobile communication when device has lost network access.&lt;/li&gt;
  &lt;li&gt;The end-to-end argument is highly specific to the applications, sometimes it makes sense to allow lower- level systems to implement functions of the application (performance aspects section of the paper).&lt;/li&gt;
  &lt;li&gt;The end-to-end argument is not valid is many modern day applications like HTTP caches, P2P net- works, NATs etc. It is indeed important to ask “Is the end-to-end argument applicable in the applica- tion?”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;The implications of violating end-to-end argument; HTTP caches - made the HTTP protocol substan- tially more complicated. Are the benefits worth the complexity?&lt;/li&gt;
  &lt;li&gt;Can the end-to-end argument expose security vulnerability in applications? Consider this example: A malicious packet is sent through a network, the client receives it and does not have anti-malware installed. The job of the network is to transmit the data, no questions asked. Wouldn’t it have been better to prevent the network transfer altogether?&lt;/li&gt;
  &lt;li&gt;Are there any systems where the end-to-end argument does not apply? – P2P connections, UDP etc.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 14 Jan 2022 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/computer%20science/system%20design/end-to-end%20principle/software%20engineering/2022/01/14/end-to-end.html</link>
        <guid isPermaLink="true">http://localhost:4000/computer%20science/system%20design/end-to-end%20principle/software%20engineering/2022/01/14/end-to-end.html</guid>
        
        <category>Compter Science</category>
        
        <category>System Design</category>
        
        <category>End-to-End Principle</category>
        
        <category>Software Engineering</category>
        
        
        <category>Computer Science</category>
        
        <category>System Design</category>
        
        <category>End-to-End Principle</category>
        
        <category>Software Engineering</category>
        
      </item>
    
  </channel>
</rss>